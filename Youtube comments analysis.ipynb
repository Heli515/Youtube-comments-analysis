{"cells":[{"cell_type":"markdown","source":["### Youtube comments analysis\n\nIn this project, a dataset of user comments for youtube videos related to animals or pets was analyzed. An attempt was made to identify cat or dog owners based on these comments. Topics important to dog or cat owners was studied, and finally, the video creators with the most viewers that are cat or dog owners was identified."],"metadata":{}},{"cell_type":"markdown","source":["#### 0. Data Exploration and Cleaning"],"metadata":{}},{"cell_type":"code","source":["# read data\ndf = spark.read.load(\"/FileStore/tables/animals_comments__1_-034b5.csv\", format='csv', header = True, inferSchema = True)\ndf.show(10)\nprint('Raw data size', df.count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+------+--------------------+\n        creator_name|userid|             comment|\n+--------------------+------+--------------------+\n        Doug The Pug|  87.0|I shared this to ...|\n        Doug The Pug|  87.0|  Super cute  üòÄüêïüê∂|\n         bulletproof| 530.0|stop saying get e...|\n       Meu Zool√≥gico| 670.0|Tenho uma jiboia ...|\n              ojatro|1031.0|I wanna see what ...|\n     Tingle Triggers|1212.0|Well shit now Im ...|\nHope For Paws - O...|1806.0|when I saw the en...|\nHope For Paws - O...|2036.0|Holy crap. That i...|\n          Life Story|2637.0|Ê≠¶Âô®„ÅØ„ÇØ„Ç®„Çπ„Éà„ÅßË≤∞„Åà„Çã„Çì„Åò„ÇÉ„Å™„ÅÑ„Çì...|\n       Brian Barczyk|2698.0|Call the teddy Larry|\n+--------------------+------+--------------------+\nonly showing top 10 rows\n\n(&apos;Raw data size&apos;, 5820035)\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# find user with preference of dog and cat\n# 'I have a dog' contains 'I have dogs' and typo\nfrom pyspark.sql.functions import size,col,count,when\nfrom pyspark.sql.types import *\n\n# Select potential pet owners\ncond = (df[\"comment\"].like(\"%my dog%\") | df[\"comment\"].like(\"%I have a dog%\") | df[\"comment\"].like(\"%my dogs%\") | df[\"comment\"].like(\"%I have dog%\")\n        | df[\"comment\"].like(\"%my cat%\") | df[\"comment\"].like(\"%my cats%\") | df[\"comment\"].like(\"%I have a cat%\") | df[\"comment\"].like(\"%I have cat%\") \n        | df[\"comment\"].like(\"%my puppy%\") | df[\"comment\"].like(\"%my puppies%\") | df[\"comment\"].like(\"%my kitty%\") | df[\"comment\"].like(\"%my kitties%\") \n        | df[\"comment\"].like(\"%I have a kitty%\") | df[\"comment\"].like(\"%I have kitties%\") | df[\"comment\"].like(\"%I have a puppy%\") | df[\"comment\"].like(\"%I have puppies%\"))\n\ndf_clean = df.withColumn('dog_cat',  cond)\n\n# Data cleaning: remove NULL \nfor colume in df_clean.columns:\n  df_clean=df_clean.filter(df_clean[colume].isNotNull())\n#label 1: cat and dog owner; label 0: Non pet owner\ndf_clean = df_clean.withColumn('label', col(\"dog_cat\").cast(IntegerType()).cast('double')) "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["##### 0.1 Tokenize\nIn this step, comments are tokenized into a list of words"],"metadata":{}},{"cell_type":"code","source":["# data preprocessing \nfrom pyspark.ml.feature import RegexTokenizer\n\nregexTokenizer = RegexTokenizer(inputCol=\"comment\", outputCol=\"tokenized\", pattern=\"\\\\W\")\ndf_clean = regexTokenizer.transform(df_clean)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["##### 0.2 Remove stop words\nIn this step, stop words are dropped since they don't have any meanings"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover\n\n# Define a list of stop words or use default list\nremover = StopWordsRemover()\nstopwords = remover.getStopWords() \n\n# Display some of the stop words\nstopwords[:10]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">10</span><span class=\"ansired\">]: </span>\n[u&apos;i&apos;,\n u&apos;me&apos;,\n u&apos;my&apos;,\n u&apos;myself&apos;,\n u&apos;we&apos;,\n u&apos;our&apos;,\n u&apos;ours&apos;,\n u&apos;ourselves&apos;,\n u&apos;you&apos;,\n u&apos;your&apos;]\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Specify input/output columns\nremover.setInputCol(\"tokenized\")\nremover.setOutputCol(\"vector_no_stopw\")\n\n# Transform existing dataframe with the StopWordsRemover\ndf_clean = remover.transform(df_clean)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# This step was supposed to remove the rows where 'vector_stemmed' are very short or empty array\n# Array length less than 4 (4 is hard coded) are removed\n\ndf_clean = df_clean.where(size(col('vector_no_stopw')) > 4)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["##### 0.3 Stemming"],"metadata":{}},{"cell_type":"code","source":["%sh /home/ubuntu/databricks/python/bin/pip install nltk"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Collecting nltk\n  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\nRequirement already satisfied: six in /usr/lib/python2.7/dist-packages (from nltk) (1.10.0)\nBuilding wheels for collected packages: nltk\n  Running setup.py bdist_wheel for nltk: started\n  Running setup.py bdist_wheel for nltk: finished with status &apos;done&apos;\n  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\nSuccessfully built nltk\nInstalling collected packages: nltk\nSuccessfully installed nltk-3.3\nYou are using pip version 10.0.1, however version 18.1 is available.\nYou should consider upgrading via the &apos;pip install --upgrade pip&apos; command.\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Import stemmer library\nfrom nltk.stem.porter import *\n\n# Instantiate stemmer object\nstemmer = PorterStemmer()\n\n# Create stemmer python function\ndef stem(in_vec):\n    out_vec = []\n    for t in in_vec:\n        t_stem = stemmer.stem(t)\n        if len(t_stem) > 2:\n            out_vec.append(t_stem)       \n    return out_vec\n\n# Create user defined function for stemming with return type Array<String>\nfrom pyspark.sql.types import *\nstemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))\n\n# Create new column with vectors containing the stemmed tokens \ndf_clean = df_clean.withColumn(\"vector_stemmed\", stemmer_udf(\"vector_no_stopw\"))  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["#### 1. Build the classifier \nIn this step, a classification model was build to identify cat or dog owner from their comments using logistic regression.\nTF-IDF was used to vectorize text. TF-IDF is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus.\n<br>\n<br> Results: \n<br> Training set areaUnderROC: 0.94326497772\n<br> Testing set areaUnderROC 0.93922450574\n<br> Training set accuracy: 0.900138546521\n<br> Testing set accuracy 0.894351918899"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF\nfrom pyspark.sql.functions import col,size,count,when,isnan\nfrom pyspark.sql import *\nfrom functools import reduce\n\ndf_clean.na.drop()\nhashingTF = HashingTF(inputCol=\"vector_stemmed\", outputCol=\"tf\", numFeatures=200)\nfeaturizedData = hashingTF.transform(df_clean)\nfeaturizedData.na.drop()\n\nfeaturizedData.withColumn('userid', col('userid').cast('float').cast(IntegerType()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">15</span><span class=\"ansired\">]: </span>DataFrame[creator_name: string, userid: int, comment: string, dog_cat: boolean, label: double, tokenized: array&lt;string&gt;, vector_no_stopw: array&lt;string&gt;, vector_stemmed: array&lt;string&gt;, tf: vector]\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["idf = IDF(inputCol=\"tf\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData)\nrescaledData.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+------+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n        creator_name|userid|             comment|dog_cat|label|           tokenized|     vector_no_stopw|      vector_stemmed|                  tf|            features|\n+--------------------+------+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n         bulletproof| 530.0|stop saying get e...|  false|  0.0|[stop, saying, ge...|[stop, saying, ge...|[stop, say, get, ...|(200,[14,39,44,52...|(200,[14,39,44,52...|\n       Meu Zool√≥gico| 670.0|Tenho uma jiboia ...|  false|  0.0|[tenho, uma, jibo...|[tenho, uma, jibo...|[tenho, uma, jibo...|(200,[22,138,157]...|(200,[22,138,157]...|\n              ojatro|1031.0|I wanna see what ...|  false|  0.0|[i, wanna, see, w...|[wanna, see, happ...|[wanna, see, happ...|(200,[6,45,72,115...|(200,[6,45,72,115...|\nHope For Paws - O...|1806.0|when I saw the en...|  false|  0.0|[when, i, saw, th...|[saw, end, said, ...|[saw, end, said, ...|(200,[13,34,37,48...|(200,[13,34,37,48...|\nHope For Paws - O...|2036.0|Holy crap. That i...|  false|  0.0|[holy, crap, that...|[holy, crap, quit...|[holi, crap, quit...|(200,[14,17,37,42...|(200,[14,17,37,42...|\nHope For Paws - O...|2911.0|That mother cat l...|  false|  0.0|[that, mother, ca...|[mother, cat, loo...|[mother, cat, loo...|(200,[19,56,74,91...|(200,[19,56,74,91...|\nHope For Paws - O...|2911.0|Its people like H...|  false|  0.0|[its, people, lik...|[people, like, ho...|[peopl, like, hop...|(200,[0,53,125,13...|(200,[0,53,125,13...|\n   Talking Kitty Cat|2911.0|steve: No wet foo...|  false|  0.0|[steve, no, wet, ...|[steve, wet, food...|[steve, wet, food...|(200,[14,22,49,53...|(200,[14,22,49,53...|\n    Brave Wilderness|3224.0|Dont call this a ...|  false|  0.0|[dont, call, this...|[dont, call, chal...|[dont, call, chal...|(200,[3,58,109,14...|(200,[3,58,109,14...|\n    Brave Wilderness|3466.0|there is no safe ...|  false|  0.0|[there, is, no, s...|[safe, way, hold,...|[safe, way, hold,...|(200,[13,42,53,87...|(200,[13,42,53,87...|\n    Brave Wilderness|3466.0|Red before yellow...|  false|  0.0|[red, before, yel...|[red, yellow, kil...|[red, yellow, kil...|(200,[28,91,110,1...|(200,[28,91,110,1...|\n       Brian Barczyk|3589.0|Im 7 and I love y...|  false|  0.0|[im, 7, and, i, l...|[im, 7, love, vid...|[love, videosand,...|(200,[1,40,106],[...|(200,[1,40,106],[...|\n         Info Marvel|4504.0|Falto deadpool en...|  false|  0.0|[falto, deadpool,...|[falto, deadpool,...|[falto, deadpool,...|(200,[63,106,148]...|(200,[63,106,148]...|\n     Gohan The Husky|4533.0|2:35 Why do you l...|  false|  0.0|[2, 35, why, do, ...|[2, 35, leave, tv...|        [leav, home]|(200,[165,181],[1...|(200,[165,181],[3...|\n   Kitsune Kreations|8857.0|I will not put ly...|  false|  0.0|[i, will, not, pu...|[put, lyrics, m, ...|[put, lyric, glad...|(200,[17,18,62,65...|(200,[17,18,62,65...|\n          Angel Hawk|8857.0|Is Vural a Serval...|  false|  0.0|[is, vural, a, se...|[vural, serval, h...|[vural, serval, h...|(200,[55,109,130,...|(200,[55,109,130,...|\n   Kitsune Kreations|8857.0|Part 36 Graystrip...|  false|  0.0|[part, 36, grayst...|[part, 36, grayst...|[part, graystrip,...|(200,[22,51,66,90...|(200,[22,51,66,90...|\n        Abbey Normal|9312.0|did the animal co...|  false|  0.0|[did, the, animal...|[animal, come, as...|[anim, come, ask,...|(200,[1,81,112,11...|(200,[1,81,112,11...|\n        HoppingHammy|9431.0|its ok girl we wo...|  false|  0.0|[its, ok, girl, w...|[ok, girl, rather...|[girl, rather, wa...|(200,[29,37,54,77...|(200,[29,37,54,77...|\n  Taylor Nicole Dean|9431.0|omg you can see c...|  false|  0.0|[omg, you, can, s...|[omg, see, casey,...|[omg, see, casey,...|(200,[60,77,113,1...|(200,[60,77,113,1...|\n+--------------------+------+--------------------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["# Cat or dog owner are only a small portion of the entire dataset. Building a claasification model using this imbalanced dataset would result in model having a tendency to predict 'non cat or dog owner'. To solve this problem, we randomly selected  some samples from 'non cat or dog owner' with the same size as 'cat or dog owner'.\n\n# The selected data prepared for training are radomly splitted into 80 and 20 percent. 80% for training and the rest 20% for testing\npet = rescaledData.filter(\"label=1.0\")\npet_train, pet_test = pet.randomSplit([0.8, 0.2])\nnopet = rescaledData.filter(\"label=0.0\")\nsampleRatio = float(pet.count()) / float(nopet.count())\nsample_nopet = nopet.sample(False, sampleRatio)\ndf_sample = pet.unionAll(sample_nopet)\nsample_nopet_train, sample_nopet_test = sample_nopet.randomSplit([0.8, 0.2])\n\ndf_train = pet_train.unionAll(sample_nopet_train)\ndf_test = pet_test.unionAll(sample_nopet_test)\nprint ('training size',df_train.count())\nprint ('testing size',df_test.count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(&apos;training size&apos;, 55577)\n(&apos;testing size&apos;, 13810)\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["# In this step, use 5-fold crossvalidation to select the best l2 parameter, best model will be saved as 'best_model'\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n\nlr = LogisticRegression(maxIter=10,featuresCol='features', labelCol='label')\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.regParam, [0.01, 0.1, 0.2, 0.4]) \\\n    .build()\n\nevaluator=BinaryClassificationEvaluator()\ncrossval = CrossValidator(estimator = lr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=5)\n\ncvModel = crossval.fit(df_train)\nbest_model = cvModel.bestModel\ntrainingSummary = best_model.summary"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# #Save trained model\npath = \"/FileStore/tables/PythonLDA/\"\n\nbest_model.save(path + 'best_model')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-393305162559119&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      2</span> path <span class=\"ansiyellow\">=</span> <span class=\"ansiblue\">&quot;/FileStore/tables/PythonLDA/&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 4</span><span class=\"ansiyellow\"> </span>best_model<span class=\"ansiyellow\">.</span>save<span class=\"ansiyellow\">(</span>path <span class=\"ansiyellow\">+</span> <span class=\"ansiblue\">&apos;best_model&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/util.py</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, path)</span>\n<span class=\"ansigreen\">    202</span>     <span class=\"ansigreen\">def</span> save<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> path<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    203</span>         <span class=\"ansiblue\">&quot;&quot;&quot;Save this ML instance to the given path, a shortcut of &apos;write().save(path)&apos;.&quot;&quot;&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 204</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>write<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>save<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    205</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    206</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/util.py</span> in <span class=\"ansicyan\">save</span><span class=\"ansiblue\">(self, path)</span>\n<span class=\"ansigreen\">    163</span>         <span class=\"ansigreen\">if</span> <span class=\"ansigreen\">not</span> isinstance<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">,</span> basestring<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    164</span>             <span class=\"ansigreen\">raise</span> TypeError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;path should be a basestring, got type %s&quot;</span> <span class=\"ansiyellow\">%</span> type<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 165</span><span class=\"ansiyellow\">         </span>self<span class=\"ansiyellow\">.</span>_jwrite<span class=\"ansiyellow\">.</span>save<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    166</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    167</span>     <span class=\"ansigreen\">def</span> overwrite<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    327</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 328</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    329</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    330</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o2911.save.\n: java.io.IOException: Path /FileStore/tables/PythonLDA/best_model already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:503)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:102)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["# If model has been already trained, load from path\nfrom pyspark.ml.classification import LogisticRegressionModel\n\npath = \"FileStore/tables/PythonLDA/\"\nbest_model = LogisticRegressionModel.load(path + 'best_model')\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["# trainingSummary = best_model.summary\nprediction_train = best_model.transform(df_train)\nprediction_test = best_model.transform(df_test)\naccuracy_train = prediction_train.filter(prediction_train.label == prediction_train.prediction).count()/float(df_train.count())\naccuracy_test = prediction_test.filter(prediction_test.label == prediction_test.prediction).count()/float(df_test.count())\n\nprint('Training set areaUnderROC: ' + str(evaluator.evaluate(prediction_train)))\nprint('Testing set areaUnderROC ' + str(evaluator.evaluate(prediction_test)))\nprint('Training set accuracy: ' + str(accuracy_train))\nprint('Testing set accuracy ' + str(accuracy_test))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Training set areaUnderROC: 0.94326497772\nTesting set areaUnderROC 0.93922450574\nTraining set accuracy: 0.900138546521\nTesting set accuracy 0.894351918899\n</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["#### 2. Classify All The Users\nWe can now apply the cat/dog classifiers to all the other users in the dataset.\nThe ratio of predicted cat or dog owners are around 14% of the entire population"],"metadata":{}},{"cell_type":"code","source":["#Apply trained model to the entire dataset\nprediction = best_model.transform(rescaledData)\n\ntotal_pet_owner = prediction.filter(\"prediction = 1.0\").count()\ntotal_population = df.select(\"userid\").distinct().count()\npet_owner_ratio = float(total_pet_owner)/float(total_population)\nprint('total_pet_owner :',total_pet_owner)\nprint('total_population :',total_population)\nprint('pet_owner_ratio :',pet_owner_ratio)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(&apos;total_pet_owner :&apos;, 373778)\n(&apos;total_population :&apos;, 2537174)\n(&apos;pet_owner_ratio :&apos;, 0.1473206015827058)\n</div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["#### 3. Get insigts of Users\nIn this part, we use LDA to analyze the potential topics that cat or dog owners are also interested in.\n\nThe list of potential topics they are interested are:\n<br>Fish \n<br>Rabbit \n<br>Chiken \n<br>Snake\n<br>Deer\n<br>Horse\n<br>Hamster\n<br>Train"],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.linalg import Vector, Vectors\nfrom pyspark.ml.clustering import LDA\nfrom pyspark.ml.feature import CountVectorizer, CountVectorizerModel\n\npet_owner = prediction.filter(\"prediction = 1.0\").select('userid','vector_stemmed')\n\ncv = CountVectorizer(inputCol=\"vector_stemmed\", outputCol=\"features\",\n                     minTF=2, # minium number of times a word must appear in a document\n                     minDF=4) # minimun number of documents a word must appear in\n\ncountVectorModel = cv.fit(pet_owner)\n\ncountVectors = (countVectorModel\n                .transform(pet_owner)\n                .select(\"userid\", \"features\").cache())\n\nprint(len(countVectorModel.vocabulary))  # how many documents, vocab size\n\nnumTopics = 10 # number of topics\n\nlda = LDA(k = numTopics,\n          maxIter = 50 # number of iterations\n          )\n\nldaModel = lda.fit(countVectors)\n\n\n# Print topics and top-weighted terms\ntopics = ldaModel.describeTopics(maxTermsPerTopic=20)\nvocabArray = countVectorModel.vocabulary\n\nListOfIndexToWords = udf(lambda wl: list([vocabArray[w] for w in wl]))\nFormatNumbers = udf(lambda nl: [\"{:1.4f}\".format(x) for x in nl])\n\ntopics.select(ListOfIndexToWords(topics.termIndices).alias('words')).show(truncate=False, n=numTopics)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">27746\n+---------------------------------------------------------------------------------------------------------------------------------------------+\nwords                                                                                                                                        |\n+---------------------------------------------------------------------------------------------------------------------------------------------+\n[thank, realli, help, tank, much, fish, turtl, nigga, sweet, carri, hola, cocain, infochammel, tiger, viralhog, jump, ruin, donat, buy, lion]|\n[cat, like, fuck, big, great, babi, meow, rabbit, box, white, fish, bite, look, chicken, tree, robin, hand, one, purr, rescu]                |\n[anim, like, get, dont, know, peopl, want, one, time, year, kitten, think, got, name, also, good, live, take, thing, food]                   |\n[video, kitti, loki, snake, deer, nice, boot, entri, sound, watch, made, sylvest, chi, pee, mishka, chuy, mine, steve, swim, tweet]          |\n[see, hors, use, eat, human, watch, beauti, walk, that, talk, eye, life, game, hate, funko, omg, way, toy, robin, wish]                      |\n[love, one, look, guy, hope, water, giveaway, win, hamster, call, girl, vid, sleep, amaz, boy, fox, intro, bath, staci, bed]                 |\n[pleas, que, cute, man, por, con, sonic, era, para, mucho, una, todo, est, pero, ador, laugh, como, brush, super, desd]                      |\n[get, pit, dog, littl, cri, play, bull, day, channel, back, servic, leg, ear, vlog, kid, run, pack, famili, subscrib, awesom]                |\n[need, puppi, dog, pet, pitbul, breed, die, poor, best, save, owner, plz, mom, never, put, new, tail, hit, like, leav]                       |\n[dog, train, peopl, make, happi, treat, kill, owner, rescu, gohan, abus, breed, bad, attack, coyot, fight, bark, aggress, sad, person]       |\n+---------------------------------------------------------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["#### 4. Identify Creators With Cat And Dog Owners In The Audience\n\n'The Dodo' is the creator with largest distinct cat or dog owner audience population, followed by 'brave wilderness' and 'hope for paws'"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct\ntmp = prediction.filter(\"prediction = 1.0\")\ntmp.groupBy('creator_name').agg(countDistinct('userid')).sort('count(DISTINCT userid)',ascending= False).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+----------------------+\n        creator_name|count(DISTINCT userid)|\n+--------------------+----------------------+\n            The Dodo|                 26027|\n    Brave Wilderness|                 17711|\nHope For Paws - O...|                 14561|\n        Robin Seplut|                 13890|\n  Taylor Nicole Dean|                 11294|\n           Vet Ranch|                 11039|\n    Cole &amp; Marmalade|                 10045|\n     Gohan The Husky|                 10017|\n       Brian Barczyk|                  9594|\n     Viktor Larkhill|                  7572|\n        Paws Channel|                  7087|\n          stacyvlogs|                  6571|\nGone to the Snow ...|                  6449|\n   Talking Kitty Cat|                  5944|\nZak Georges Dog T...|                  5442|\n           meow meow|                  3431|\n             ViralBe|                  3386|\n  The Pet Collective|                  3291|\n    SlideShow ForFun|                  3231|\n         Info Marvel|                  3188|\n+--------------------+----------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["#### 5. Analysis and Future work"],"metadata":{}},{"cell_type":"markdown","source":["According to the work, around 14.5% of the total user who commented on Youtube in this dataset are dog or cat owners. The potential topics they are interested in are include Fish, Rabbit, Chiken, Snake, Deer, Horse, Hamster, Train, etc. Videos related to these topics could be promoted to these cat or dog owners. Also, 'The Dodo', 'brave wilderness' and 'hope for paws' are the top three creators with largest distinct cat or dog owner audience population. Ads targeting cat or dog owners will potentially have the biggest payback cooperating with these creators.  \n\nFor future work, this work could be improved in the following aspects:\n<br> 1. To select the cat or dog owners more accurately, the breeds of cats and dogs should be considered\n<br> 2. A pipeline could be built to replace the stemming, tokenize, tf-idf process\n<br> 3. Stop words could be further removed from LDA results"],"metadata":{}}],"metadata":{"name":"spark hw2","notebookId":4431224514809288},"nbformat":4,"nbformat_minor":0}
